{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4z1z_yoAKXPk"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import string\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0hMuj9yWY9P"
      },
      "source": [
        "**Data Extraction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mR5efVNZKjMY"
      },
      "outputs": [],
      "source": [
        "zip_path='/content/drive/MyDrive/DeepLearningTask3/SciFi.zip'\n",
        "extract_path = \"/content/task3\"\n",
        "if not os.path.exists(extract_path):\n",
        "    os.makedirs(extract_path)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_obj:\n",
        "  zip_obj.extractall(extract_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIphsZPsK0lg",
        "outputId": "81390674-0766-4441-9498-a7aa1d64e33e"
      },
      "outputs": [],
      "source": [
        "text=''\n",
        "with open('/content/task3/internet_archive_scifi_v3.txt', 'r') as f:\n",
        "    chunk_size = 1024 # set the chunk size to be read\n",
        "    while True:\n",
        "        data = f.read(chunk_size)\n",
        "        if not data:\n",
        "            break\n",
        "        # processing the data\n",
        "        text=text+data\n",
        "        print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvAdOo3lLwO1",
        "outputId": "c416b60b-82c7-472b-be16-d2d6f5ca72d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of text: 149326361 characters\n"
          ]
        }
      ],
      "source": [
        "print(f'Length of text: {len(list(text))} characters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDsf1V2EWfkL"
      },
      "source": [
        "**Data Pre-processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cueUTA5tNDy"
      },
      "outputs": [],
      "source": [
        "#changing text to lower text and removing punctuations\n",
        "text=text.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khuJuFk-NLAH",
        "outputId": "f9bed309-e299-4045-e34d-85837a978a1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "49 unique characters\n"
          ]
        }
      ],
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOdt73a5hzVe",
        "outputId": "6752800d-1486-4ebb-920a-625cd723973e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[' ', '!', '\"', '#', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ],
      "source": [
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuhVDGfkswRt"
      },
      "outputs": [],
      "source": [
        "book = []\n",
        "with open('/content/task3/internet_archive_scifi_v3.txt') as pdf:\n",
        "    for line in pdf:\n",
        "        book.append(line)\n",
        "book[0] = book[0][:len(book[0])//1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCW9RI0MOEgR",
        "outputId": "192cd452-2bbf-41a3-8e6f-89d241f4ee71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "march  all stories new and complete publisher editor if is published bimonthly by quinn publishing company inc .  kingston new york .  volume  no .   .  copyright  by quinn publishing company inc .  a\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "punctuations = string.punctuation\n",
        "punctuations += '1234567890'\n",
        "eol = '.!?'\n",
        "\n",
        "cleaned_book = []\n",
        "for line in book:\n",
        "    cleaned_line = ''\n",
        "    for char in line:\n",
        "        if char in eol:\n",
        "            cleaned_line += ' . '\n",
        "            continue\n",
        "        if char in punctuations or char == '\\n':\n",
        "            continue\n",
        "        cleaned_line += char\n",
        "    cleaned_line = cleaned_line.lower()\n",
        "    cleaned_book.append(cleaned_line)\n",
        "\n",
        "all_text = ' \\n '.join(cleaned_book)\n",
        "print(all_text[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "NTA_D8c6yMGp",
        "outputId": "ff55a566-6b44-4d3a-8432-3d894e26de98"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'march  all stories new and complete publisher editor if is published bimonthly by quinn publishing company inc .  kingston new york .  volume  no .   .  copyright  by quinn publishing company inc .  application for entry as second class matter at post office buffalo new york pending .  subscription  for  issues in u . s .  and possessions canada  for  issues elsewhere  .  aiiow four weeks for change of address .  all stories appearing in this magazine are fiction .  any similarity to actual pers'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_text[:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkisFCuNyfDR",
        "outputId": "76bdcd56-442c-4dfa-c678-1871298eaa12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['m', 'a', 'r', 'c', 'h', ' ', ' ', 'a', 'l', 'l']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def clean_text(txt):\n",
        "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
        "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
        "    return txt\n",
        "\n",
        "corpus = [clean_text(x) for x in all_text]\n",
        "corpus[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDzaeJF7tP_8",
        "outputId": "67f22c3d-c30a-46bb-c0ba-8bb3b58a3de6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2430,)\n",
            "['march  all stories new and complete publisher editor if is published bimonthly by quinn publishing company inc '\n",
            " '  kingston new york ' '  volume  no ' '   '\n",
            " '  copyright  by quinn publishing company inc '\n",
            " '  application for entry as second class matter at post office buffalo new york pending '\n",
            " '  subscription  for  issues in u ' ' s '\n",
            " '  and possessions canada  for  issues elsewhere  '\n",
            " '  aiiow four weeks for change of address ']\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "text_tokens = all_text.split(\".\")\n",
        "text_tokens = np.array(text_tokens)\n",
        "#text_tokens = text_tokens.reshape(len(text_tokens), 1)\n",
        "print(text_tokens.shape)\n",
        "print(text_tokens[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laBHS2TvbDsc",
        "outputId": "a129b430-03cf-4481-f20d-037c6539cc4b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[1970, 41],\n",
              " [1970, 41, 421],\n",
              " [1970, 41, 421, 228],\n",
              " [1970, 41, 421, 228, 5],\n",
              " [1970, 41, 421, 228, 5, 771],\n",
              " [1970, 41, 421, 228, 5, 771, 1971],\n",
              " [1970, 41, 421, 228, 5, 771, 1971, 772],\n",
              " [1970, 41, 421, 228, 5, 771, 1971, 772, 57],\n",
              " [1970, 41, 421, 228, 5, 771, 1971, 772, 57, 37],\n",
              " [1970, 41, 421, 228, 5, 771, 1971, 772, 57, 37, 1972],\n",
              " [1970, 41, 421, 228, 5, 771, 1971, 772, 57, 37, 1972, 1973],\n",
              " [1970, 41, 421, 228, 5, 771, 1971, 772, 57, 37, 1972, 1973, 43],\n",
              " [1970, 41, 421, 228, 5, 771, 1971, 772, 57, 37, 1972, 1973, 43, 1277],\n",
              " [1970, 41, 421, 228, 5, 771, 1971, 772, 57, 37, 1972, 1973, 43, 1277, 1278],\n",
              " [1970,\n",
              "  41,\n",
              "  421,\n",
              "  228,\n",
              "  5,\n",
              "  771,\n",
              "  1971,\n",
              "  772,\n",
              "  57,\n",
              "  37,\n",
              "  1972,\n",
              "  1973,\n",
              "  43,\n",
              "  1277,\n",
              "  1278,\n",
              "  536],\n",
              " [1970,\n",
              "  41,\n",
              "  421,\n",
              "  228,\n",
              "  5,\n",
              "  771,\n",
              "  1971,\n",
              "  772,\n",
              "  57,\n",
              "  37,\n",
              "  1972,\n",
              "  1973,\n",
              "  43,\n",
              "  1277,\n",
              "  1278,\n",
              "  536,\n",
              "  1279],\n",
              " [1974, 228],\n",
              " [1974, 228, 1280],\n",
              " [1975, 32],\n",
              " [1976, 43]]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "def get_sequence_of_tokens(corpus):\n",
        "    ## tokenization\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "    ## convert data to sequence of tokens\n",
        "    input_sequences = []\n",
        "    for line in corpus:\n",
        "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "        for i in range(1, len(token_list)):\n",
        "            n_gram_sequence = token_list[:i+1]\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "    return input_sequences, total_words\n",
        "\n",
        "inp_sequences, total_words = get_sequence_of_tokens(text_tokens)\n",
        "inp_sequences[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfKez-4HtORb"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import keras.utils as ku\n",
        "\n",
        "def generate_padded_sequences(input_sequences):\n",
        "    max_sequence_len = max([len(x) for x in input_sequences])\n",
        "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "    label = ku.to_categorical(label, num_classes=total_words)\n",
        "    return predictors, label, max_sequence_len\n",
        "\n",
        "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI2zUiBw6Epc"
      },
      "source": [
        "Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYNsC4GFbDzO",
        "outputId": "d66b10ea-a805-4596-f4ad-1a12c0cb51b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 83, 10)            47310     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 100)               44400     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 100)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4731)              477831    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 569,541\n",
            "Trainable params: 569,541\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "\n",
        "def create_model(max_sequence_len, total_words):\n",
        "    input_len = max_sequence_len - 1\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add Input Embedding Layer\n",
        "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
        "\n",
        "    # Add Hidden Layer 1 - LSTM Layer\n",
        "    model.add(LSTM(100))\n",
        "    model.add(Dropout(0.1))\n",
        "\n",
        "    # Add Output Layer\n",
        "    model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "model = create_model(max_sequence_len, total_words)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKg3EJXjbD57",
        "outputId": "f57c1cfe-085b-41d1-cb28-1949c9920bc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 6.9499 - accuracy: 0.0545\n",
            "Epoch 1: loss improved from inf to 6.94991, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 73s 91ms/step - loss: 6.9499 - accuracy: 0.0545\n",
            "Epoch 2/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 6.5701 - accuracy: 0.0612\n",
            "Epoch 2: loss improved from 6.94991 to 6.57012, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 75s 97ms/step - loss: 6.5701 - accuracy: 0.0612\n",
            "Epoch 3/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 6.4161 - accuracy: 0.0678\n",
            "Epoch 3: loss improved from 6.57012 to 6.41606, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 71s 92ms/step - loss: 6.4161 - accuracy: 0.0678\n",
            "Epoch 4/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 6.2596 - accuracy: 0.0711\n",
            "Epoch 4: loss improved from 6.41606 to 6.25963, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 72s 93ms/step - loss: 6.2596 - accuracy: 0.0711\n",
            "Epoch 5/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 6.1219 - accuracy: 0.0768\n",
            "Epoch 5: loss improved from 6.25963 to 6.12195, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 73s 94ms/step - loss: 6.1219 - accuracy: 0.0768\n",
            "Epoch 6/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 5.9887 - accuracy: 0.0799\n",
            "Epoch 6: loss improved from 6.12195 to 5.98873, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 74s 96ms/step - loss: 5.9887 - accuracy: 0.0799\n",
            "Epoch 7/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 5.8467 - accuracy: 0.0838\n",
            "Epoch 7: loss improved from 5.98873 to 5.84671, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 78s 101ms/step - loss: 5.8467 - accuracy: 0.0838\n",
            "Epoch 8/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 5.6937 - accuracy: 0.0911\n",
            "Epoch 8: loss improved from 5.84671 to 5.69369, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 75s 97ms/step - loss: 5.6937 - accuracy: 0.0911\n",
            "Epoch 9/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 5.5361 - accuracy: 0.0970\n",
            "Epoch 9: loss improved from 5.69369 to 5.53612, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 74s 95ms/step - loss: 5.5361 - accuracy: 0.0970\n",
            "Epoch 10/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 5.3806 - accuracy: 0.1028\n",
            "Epoch 10: loss improved from 5.53612 to 5.38064, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 76s 98ms/step - loss: 5.3806 - accuracy: 0.1028\n",
            "Epoch 11/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 5.2249 - accuracy: 0.1086\n",
            "Epoch 11: loss improved from 5.38064 to 5.22486, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 74s 96ms/step - loss: 5.2249 - accuracy: 0.1086\n",
            "Epoch 12/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 5.0787 - accuracy: 0.1153\n",
            "Epoch 12: loss improved from 5.22486 to 5.07873, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 74s 96ms/step - loss: 5.0787 - accuracy: 0.1153\n",
            "Epoch 13/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 4.9338 - accuracy: 0.1222\n",
            "Epoch 13: loss improved from 5.07873 to 4.93383, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 76s 98ms/step - loss: 4.9338 - accuracy: 0.1222\n",
            "Epoch 14/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 4.7942 - accuracy: 0.1293\n",
            "Epoch 14: loss improved from 4.93383 to 4.79416, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 74s 96ms/step - loss: 4.7942 - accuracy: 0.1293\n",
            "Epoch 15/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 4.6639 - accuracy: 0.1384\n",
            "Epoch 15: loss improved from 4.79416 to 4.66391, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 74s 96ms/step - loss: 4.6639 - accuracy: 0.1384\n",
            "Epoch 16/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 4.5297 - accuracy: 0.1483\n",
            "Epoch 16: loss improved from 4.66391 to 4.52972, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 75s 98ms/step - loss: 4.5297 - accuracy: 0.1483\n",
            "Epoch 17/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 4.3982 - accuracy: 0.1642\n",
            "Epoch 17: loss improved from 4.52972 to 4.39821, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 76s 98ms/step - loss: 4.3982 - accuracy: 0.1642\n",
            "Epoch 18/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 4.2835 - accuracy: 0.1761\n",
            "Epoch 18: loss improved from 4.39821 to 4.28352, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 77s 99ms/step - loss: 4.2835 - accuracy: 0.1761\n",
            "Epoch 19/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 4.1576 - accuracy: 0.1925\n",
            "Epoch 19: loss improved from 4.28352 to 4.15760, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 76s 98ms/step - loss: 4.1576 - accuracy: 0.1925\n",
            "Epoch 20/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 4.0447 - accuracy: 0.2080\n",
            "Epoch 20: loss improved from 4.15760 to 4.04466, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 76s 98ms/step - loss: 4.0447 - accuracy: 0.2080\n",
            "Epoch 21/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 3.9349 - accuracy: 0.2231\n",
            "Epoch 21: loss improved from 4.04466 to 3.93490, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 75s 97ms/step - loss: 3.9349 - accuracy: 0.2231\n",
            "Epoch 22/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 3.8263 - accuracy: 0.2368\n",
            "Epoch 22: loss improved from 3.93490 to 3.82632, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 76s 98ms/step - loss: 3.8263 - accuracy: 0.2368\n",
            "Epoch 23/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 3.7293 - accuracy: 0.2562\n",
            "Epoch 23: loss improved from 3.82632 to 3.72928, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 76s 98ms/step - loss: 3.7293 - accuracy: 0.2562\n",
            "Epoch 24/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 3.6340 - accuracy: 0.2688\n",
            "Epoch 24: loss improved from 3.72928 to 3.63402, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 78s 100ms/step - loss: 3.6340 - accuracy: 0.2688\n",
            "Epoch 25/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 3.5405 - accuracy: 0.2866\n",
            "Epoch 25: loss improved from 3.63402 to 3.54051, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 75s 97ms/step - loss: 3.5405 - accuracy: 0.2866\n",
            "Epoch 26/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 3.4595 - accuracy: 0.2989\n",
            "Epoch 26: loss improved from 3.54051 to 3.45953, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 75s 97ms/step - loss: 3.4595 - accuracy: 0.2989\n",
            "Epoch 27/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 3.3762 - accuracy: 0.3106\n",
            "Epoch 27: loss improved from 3.45953 to 3.37624, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 75s 97ms/step - loss: 3.3762 - accuracy: 0.3106\n",
            "Epoch 28/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 3.2992 - accuracy: 0.3234\n",
            "Epoch 28: loss improved from 3.37624 to 3.29921, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 75s 96ms/step - loss: 3.2992 - accuracy: 0.3234\n",
            "Epoch 29/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 3.2255 - accuracy: 0.3395\n",
            "Epoch 29: loss improved from 3.29921 to 3.22550, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 77s 99ms/step - loss: 3.2255 - accuracy: 0.3395\n",
            "Epoch 30/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 3.1622 - accuracy: 0.3460\n",
            "Epoch 30: loss improved from 3.22550 to 3.16218, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 76s 98ms/step - loss: 3.1622 - accuracy: 0.3460\n",
            "Epoch 31/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 3.0929 - accuracy: 0.3593\n",
            "Epoch 31: loss improved from 3.16218 to 3.09295, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 76s 98ms/step - loss: 3.0929 - accuracy: 0.3593\n",
            "Epoch 32/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 3.0294 - accuracy: 0.3687\n",
            "Epoch 32: loss improved from 3.09295 to 3.02943, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 77s 100ms/step - loss: 3.0294 - accuracy: 0.3687\n",
            "Epoch 33/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 2.9704 - accuracy: 0.3796\n",
            "Epoch 33: loss improved from 3.02943 to 2.97043, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 75s 97ms/step - loss: 2.9704 - accuracy: 0.3796\n",
            "Epoch 34/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 2.9124 - accuracy: 0.3902\n",
            "Epoch 34: loss improved from 2.97043 to 2.91239, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 78s 101ms/step - loss: 2.9124 - accuracy: 0.3902\n",
            "Epoch 35/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 2.8551 - accuracy: 0.3995\n",
            "Epoch 35: loss improved from 2.91239 to 2.85505, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 77s 99ms/step - loss: 2.8551 - accuracy: 0.3995\n",
            "Epoch 36/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 2.8083 - accuracy: 0.4043\n",
            "Epoch 36: loss improved from 2.85505 to 2.80833, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 76s 99ms/step - loss: 2.8083 - accuracy: 0.4043\n",
            "Epoch 37/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 2.7622 - accuracy: 0.4136\n",
            "Epoch 37: loss improved from 2.80833 to 2.76217, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 76s 98ms/step - loss: 2.7622 - accuracy: 0.4136\n",
            "Epoch 38/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 2.7067 - accuracy: 0.4228\n",
            "Epoch 38: loss improved from 2.76217 to 2.70668, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 75s 97ms/step - loss: 2.7067 - accuracy: 0.4228\n",
            "Epoch 39/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 2.6638 - accuracy: 0.4313\n",
            "Epoch 39: loss improved from 2.70668 to 2.66384, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 75s 97ms/step - loss: 2.6638 - accuracy: 0.4313\n",
            "Epoch 40/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 2.6202 - accuracy: 0.4411\n",
            "Epoch 40: loss improved from 2.66384 to 2.62018, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 78s 101ms/step - loss: 2.6202 - accuracy: 0.4411\n",
            "Epoch 41/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 2.5754 - accuracy: 0.4472\n",
            "Epoch 41: loss improved from 2.62018 to 2.57538, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 76s 98ms/step - loss: 2.5754 - accuracy: 0.4472\n",
            "Epoch 42/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 2.5380 - accuracy: 0.4521\n",
            "Epoch 42: loss improved from 2.57538 to 2.53799, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 75s 97ms/step - loss: 2.5380 - accuracy: 0.4521\n",
            "Epoch 43/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 2.4960 - accuracy: 0.4595\n",
            "Epoch 43: loss improved from 2.53799 to 2.49599, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 75s 97ms/step - loss: 2.4960 - accuracy: 0.4595\n",
            "Epoch 44/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 2.4630 - accuracy: 0.4671\n",
            "Epoch 44: loss improved from 2.49599 to 2.46303, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 77s 99ms/step - loss: 2.4630 - accuracy: 0.4671\n",
            "Epoch 45/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 2.4305 - accuracy: 0.4730\n",
            "Epoch 45: loss improved from 2.46303 to 2.43053, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 80s 103ms/step - loss: 2.4305 - accuracy: 0.4730\n",
            "Epoch 46/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 2.3962 - accuracy: 0.4805\n",
            "Epoch 46: loss improved from 2.43053 to 2.39624, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 76s 99ms/step - loss: 2.3962 - accuracy: 0.4805\n",
            "Epoch 47/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 2.3615 - accuracy: 0.4845\n",
            "Epoch 47: loss improved from 2.39624 to 2.36148, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 76s 99ms/step - loss: 2.3615 - accuracy: 0.4845\n",
            "Epoch 48/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 2.3315 - accuracy: 0.4874\n",
            "Epoch 48: loss improved from 2.36148 to 2.33153, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 76s 98ms/step - loss: 2.3315 - accuracy: 0.4874\n",
            "Epoch 49/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 2.2968 - accuracy: 0.4974\n",
            "Epoch 49: loss improved from 2.33153 to 2.29678, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 76s 99ms/step - loss: 2.2968 - accuracy: 0.4974\n",
            "Epoch 50/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 2.2686 - accuracy: 0.5021\n",
            "Epoch 50: loss improved from 2.29678 to 2.26857, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 78s 101ms/step - loss: 2.2686 - accuracy: 0.5021\n",
            "Epoch 51/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 2.2390 - accuracy: 0.5045\n",
            "Epoch 51: loss improved from 2.26857 to 2.23904, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 76s 98ms/step - loss: 2.2390 - accuracy: 0.5045\n",
            "Epoch 52/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 2.2154 - accuracy: 0.5104\n",
            "Epoch 52: loss improved from 2.23904 to 2.21541, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 76s 98ms/step - loss: 2.2154 - accuracy: 0.5104\n",
            "Epoch 53/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 2.1816 - accuracy: 0.5196\n",
            "Epoch 53: loss improved from 2.21541 to 2.18159, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 75s 97ms/step - loss: 2.1816 - accuracy: 0.5196\n",
            "Epoch 54/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 2.1596 - accuracy: 0.5225\n",
            "Epoch 54: loss improved from 2.18159 to 2.15957, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 76s 99ms/step - loss: 2.1596 - accuracy: 0.5225\n",
            "Epoch 55/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 2.1307 - accuracy: 0.5234\n",
            "Epoch 55: loss improved from 2.15957 to 2.13074, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 78s 101ms/step - loss: 2.1307 - accuracy: 0.5234\n",
            "Epoch 56/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 2.1042 - accuracy: 0.5319\n",
            "Epoch 56: loss improved from 2.13074 to 2.10423, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 76s 99ms/step - loss: 2.1042 - accuracy: 0.5319\n",
            "Epoch 57/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 2.0841 - accuracy: 0.5344\n",
            "Epoch 57: loss improved from 2.10423 to 2.08410, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 77s 99ms/step - loss: 2.0841 - accuracy: 0.5344\n",
            "Epoch 58/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 2.0686 - accuracy: 0.5377\n",
            "Epoch 58: loss improved from 2.08410 to 2.06859, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 78s 101ms/step - loss: 2.0686 - accuracy: 0.5377\n",
            "Epoch 59/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 2.0443 - accuracy: 0.5409\n",
            "Epoch 59: loss improved from 2.06859 to 2.04432, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 77s 100ms/step - loss: 2.0443 - accuracy: 0.5409\n",
            "Epoch 60/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 2.0181 - accuracy: 0.5480\n",
            "Epoch 60: loss improved from 2.04432 to 2.01815, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 78s 101ms/step - loss: 2.0181 - accuracy: 0.5480\n",
            "Epoch 61/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.9971 - accuracy: 0.5511\n",
            "Epoch 61: loss improved from 2.01815 to 1.99714, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 77s 100ms/step - loss: 1.9971 - accuracy: 0.5511\n",
            "Epoch 62/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.9759 - accuracy: 0.5545\n",
            "Epoch 62: loss improved from 1.99714 to 1.97593, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 78s 101ms/step - loss: 1.9759 - accuracy: 0.5545\n",
            "Epoch 63/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.9548 - accuracy: 0.5596\n",
            "Epoch 63: loss improved from 1.97593 to 1.95480, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 76s 99ms/step - loss: 1.9548 - accuracy: 0.5596\n",
            "Epoch 64/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.9372 - accuracy: 0.5621\n",
            "Epoch 64: loss improved from 1.95480 to 1.93725, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 77s 99ms/step - loss: 1.9372 - accuracy: 0.5621\n",
            "Epoch 65/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.9273 - accuracy: 0.5658\n",
            "Epoch 65: loss improved from 1.93725 to 1.92733, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 78s 101ms/step - loss: 1.9273 - accuracy: 0.5658\n",
            "Epoch 66/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.9075 - accuracy: 0.5689\n",
            "Epoch 66: loss improved from 1.92733 to 1.90752, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 78s 101ms/step - loss: 1.9075 - accuracy: 0.5689\n",
            "Epoch 67/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.8922 - accuracy: 0.5716\n",
            "Epoch 67: loss improved from 1.90752 to 1.89220, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 77s 100ms/step - loss: 1.8922 - accuracy: 0.5716\n",
            "Epoch 68/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.8636 - accuracy: 0.5769\n",
            "Epoch 68: loss improved from 1.89220 to 1.86361, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 78s 101ms/step - loss: 1.8636 - accuracy: 0.5769\n",
            "Epoch 69/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.8548 - accuracy: 0.5771\n",
            "Epoch 69: loss improved from 1.86361 to 1.85484, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 80s 103ms/step - loss: 1.8548 - accuracy: 0.5771\n",
            "Epoch 70/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.8359 - accuracy: 0.5805\n",
            "Epoch 70: loss improved from 1.85484 to 1.83588, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 78s 101ms/step - loss: 1.8359 - accuracy: 0.5805\n",
            "Epoch 71/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.8161 - accuracy: 0.5831\n",
            "Epoch 71: loss improved from 1.83588 to 1.81610, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 78s 101ms/step - loss: 1.8161 - accuracy: 0.5831\n",
            "Epoch 72/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.8042 - accuracy: 0.5867\n",
            "Epoch 72: loss improved from 1.81610 to 1.80419, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 78s 100ms/step - loss: 1.8042 - accuracy: 0.5867\n",
            "Epoch 73/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.7861 - accuracy: 0.5937\n",
            "Epoch 73: loss improved from 1.80419 to 1.78606, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 79s 103ms/step - loss: 1.7861 - accuracy: 0.5937\n",
            "Epoch 74/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.7747 - accuracy: 0.5905\n",
            "Epoch 74: loss improved from 1.78606 to 1.77466, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 79s 102ms/step - loss: 1.7747 - accuracy: 0.5905\n",
            "Epoch 75/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.7632 - accuracy: 0.5934\n",
            "Epoch 75: loss improved from 1.77466 to 1.76315, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 77s 100ms/step - loss: 1.7632 - accuracy: 0.5934\n",
            "Epoch 76/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.7453 - accuracy: 0.5958\n",
            "Epoch 76: loss improved from 1.76315 to 1.74531, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 79s 102ms/step - loss: 1.7453 - accuracy: 0.5958\n",
            "Epoch 77/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.7347 - accuracy: 0.5997\n",
            "Epoch 77: loss improved from 1.74531 to 1.73467, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 80s 103ms/step - loss: 1.7347 - accuracy: 0.5997\n",
            "Epoch 78/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.7244 - accuracy: 0.6020\n",
            "Epoch 78: loss improved from 1.73467 to 1.72443, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 80s 104ms/step - loss: 1.7244 - accuracy: 0.6020\n",
            "Epoch 79/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.7215 - accuracy: 0.6001\n",
            "Epoch 79: loss improved from 1.72443 to 1.72147, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 79s 102ms/step - loss: 1.7215 - accuracy: 0.6001\n",
            "Epoch 80/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.7036 - accuracy: 0.6028\n",
            "Epoch 80: loss improved from 1.72147 to 1.70363, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 77s 99ms/step - loss: 1.7036 - accuracy: 0.6028\n",
            "Epoch 81/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.6860 - accuracy: 0.6091\n",
            "Epoch 81: loss improved from 1.70363 to 1.68595, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 78s 100ms/step - loss: 1.6860 - accuracy: 0.6091\n",
            "Epoch 82/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.6752 - accuracy: 0.6103\n",
            "Epoch 82: loss improved from 1.68595 to 1.67517, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 79s 102ms/step - loss: 1.6752 - accuracy: 0.6103\n",
            "Epoch 83/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.6622 - accuracy: 0.6134\n",
            "Epoch 83: loss improved from 1.67517 to 1.66223, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 79s 102ms/step - loss: 1.6622 - accuracy: 0.6134\n",
            "Epoch 84/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.6565 - accuracy: 0.6150\n",
            "Epoch 84: loss improved from 1.66223 to 1.65650, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 77s 100ms/step - loss: 1.6565 - accuracy: 0.6150\n",
            "Epoch 85/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.6400 - accuracy: 0.6179\n",
            "Epoch 85: loss improved from 1.65650 to 1.64003, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 77s 100ms/step - loss: 1.6400 - accuracy: 0.6179\n",
            "Epoch 86/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.6295 - accuracy: 0.6193\n",
            "Epoch 86: loss improved from 1.64003 to 1.62947, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 79s 102ms/step - loss: 1.6295 - accuracy: 0.6193\n",
            "Epoch 87/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.6143 - accuracy: 0.6209\n",
            "Epoch 87: loss improved from 1.62947 to 1.61428, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 79s 102ms/step - loss: 1.6143 - accuracy: 0.6209\n",
            "Epoch 88/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.6118 - accuracy: 0.6245\n",
            "Epoch 88: loss improved from 1.61428 to 1.61185, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 78s 100ms/step - loss: 1.6118 - accuracy: 0.6245\n",
            "Epoch 89/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.5944 - accuracy: 0.6257\n",
            "Epoch 89: loss improved from 1.61185 to 1.59443, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 78s 102ms/step - loss: 1.5944 - accuracy: 0.6257\n",
            "Epoch 90/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.5871 - accuracy: 0.6260\n",
            "Epoch 90: loss improved from 1.59443 to 1.58712, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 78s 100ms/step - loss: 1.5871 - accuracy: 0.6260\n",
            "Epoch 91/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.5876 - accuracy: 0.6279\n",
            "Epoch 91: loss did not improve from 1.58712\n",
            "773/773 [==============================] - 80s 103ms/step - loss: 1.5876 - accuracy: 0.6279\n",
            "Epoch 92/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.5820 - accuracy: 0.6266\n",
            "Epoch 92: loss improved from 1.58712 to 1.58205, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 78s 101ms/step - loss: 1.5820 - accuracy: 0.6266\n",
            "Epoch 93/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.5552 - accuracy: 0.6342\n",
            "Epoch 93: loss improved from 1.58205 to 1.55525, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 80s 103ms/step - loss: 1.5552 - accuracy: 0.6342\n",
            "Epoch 94/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.5515 - accuracy: 0.6326\n",
            "Epoch 94: loss improved from 1.55525 to 1.55149, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 83s 108ms/step - loss: 1.5515 - accuracy: 0.6326\n",
            "Epoch 95/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.5494 - accuracy: 0.6351\n",
            "Epoch 95: loss improved from 1.55149 to 1.54940, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 82s 106ms/step - loss: 1.5494 - accuracy: 0.6351\n",
            "Epoch 96/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.5308 - accuracy: 0.6367\n",
            "Epoch 96: loss improved from 1.54940 to 1.53082, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 78s 101ms/step - loss: 1.5308 - accuracy: 0.6367\n",
            "Epoch 97/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.5388 - accuracy: 0.6333\n",
            "Epoch 97: loss did not improve from 1.53082\n",
            "773/773 [==============================] - 79s 102ms/step - loss: 1.5388 - accuracy: 0.6333\n",
            "Epoch 98/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.5134 - accuracy: 0.6381\n",
            "Epoch 98: loss improved from 1.53082 to 1.51336, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 81s 104ms/step - loss: 1.5134 - accuracy: 0.6381\n",
            "Epoch 99/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.5169 - accuracy: 0.6394\n",
            "Epoch 99: loss did not improve from 1.51336\n",
            "773/773 [==============================] - 78s 101ms/step - loss: 1.5169 - accuracy: 0.6394\n",
            "Epoch 100/100\n",
            "773/773 [==============================] - ETA: 0s - loss: 1.5110 - accuracy: 0.6417\n",
            "Epoch 100: loss improved from 1.51336 to 1.51102, saving model to ./model_checkpoints/text_generation_checkpoint.h5\n",
            "773/773 [==============================] - 78s 101ms/step - loss: 1.5110 - accuracy: 0.6417\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5b4c3bba30>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "filepath = \"./model_checkpoints/text_generation_checkpoint.h5\"\n",
        "model_checkpoint_callback = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(predictors, label, epochs=100,callbacks=[model_checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qm86n8OaWybs"
      },
      "source": [
        "**Text generator block**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bR3frqSmSQUR"
      },
      "outputs": [],
      "source": [
        "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        predicted = model.predict(token_list, verbose=0)\n",
        "        classes_x=np.argmax(predicted,axis=1)\n",
        "\n",
        "        output_word = \"\"\n",
        "        for word,index in tokenizer.word_index.items():\n",
        "            if index == classes_x:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \"+output_word\n",
        "    return seed_text.title()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pL9HWI00fMIW",
        "outputId": "67cc880b-5b44-4308-d906-50ad1ec68bf1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "#model.save('/content/drive/MyDrive/TEXT_GENERATION_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hpYX88mn6Q-"
      },
      "outputs": [],
      "source": [
        "#shutil.copytree('/content/model_checkpoints','/content/drive/MyDrive/TEXT_GENERATION_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG2kYtQrXAEE"
      },
      "source": [
        "**Generating text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqIGYhPvSQZ9",
        "outputId": "db4274ef-ca1b-43a7-dda8-6cfc88fb197d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Port Began To Swing Shut With The Dark Boy For A Woman In The Large Period Of Blue Light But The Far Of Her Mouth Youre Applying The Lab And She Was Their Part Of This Was Light And It Seemed He Was Watching And\n"
          ]
        }
      ],
      "source": [
        "print (generate_text(\"port began to swing shut\", 40, model, max_sequence_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ta_X1LZA1sZ",
        "outputId": "baf57ad5-d2c6-46e3-b0e0-a187c92cc92e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A Quite Voice In The Darkness And He Went Up Toward His Own Room But The Daughter Of Some Of Earths Better Than No People And The Shakes Closed A Next But The Dark World Of The Cone Of Hot Light And The\n"
          ]
        }
      ],
      "source": [
        "print (generate_text(\"a quite voice\", 40, model, max_sequence_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5p5LomZvbl7x",
        "outputId": "8774a905-f89d-4a56-ea23-67767ebe24b6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/TEXT_GENERATION_model_v2'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "shutil.copytree('/content/model_checkpoints','/content/drive/MyDrive/TEXT_GENERATION_checkpoints_v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8k8iTg1vSQe9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmpJ4dh5SQjS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
